{"nbformat": 4, "cells": [{"cell_type": "markdown", "metadata": {}, "source": "# LocalCart scenario part 2: Creating streaming pipelines\n\n\n## Introduction \n\nA web or mobile app will trigger events as a user navigates a web site. These clickstream events indicate when a customer logs in, adds something to a basket, completes an order, and logs out. The events are placed into configured Message Hub (Apache Kafka) that provides a scalable way to buffer the data before it is saved, analysed, and rendered. Using the instructions in [Notebook #1 - Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb) we generate clickstream events for LocalCart and send them to Message Hub to show how data can be collected offline and streamed to the cloud later. A Java app continuously feeds a simulated stream of events to Message Hub. \n\nThis notebook is divided into two parts, describing how to use pipelines to perform streaming data analysis and save data for static analysis.\n\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb2_flow.png\"></img>\n\n\n### Streaming data analysis\n\n[Example 1: Capturing clickstream events for real-time analysis](#intro_a). You can use streaming pipelines to performs event-based aggregation operations (calculate the number of currently open baskets and value of those baskets, ...) on the fly and store the results into a Redis database. The aggregated data can easily be visualized in real-time using web applications that monitor this database, as described in [Notebook#4:Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb). \n\n\n### Static data analysis\n\n[Example 2: Capturing clickstream events for static analysis](#intro_b). You can also use streaming pipelines to store clickstream events (as-is or in modified form) in flat files, which can be processed offline - either by batch processes or interactively, as outlined in [Notebook#3b: Analyze static clickstreams](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb).\n\n\nThis notebook runs on Python 2 with Spark 2.0."}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"intro_a\"></a>\n\n***\n# Example 1: Capturing clickstream events for real-time analysis\n***\n\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/streaming_analysis.png\"></img>\n\n\nIn this first example you will create a pipeline that ingests `login`, `add_to_basket` and `checkout` clickstream events, aggregates them according to our business needs and stores the aggregated data in a Redis database, which will be monitored by a real-time dashboard:\n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/MARMARMAR_result.png'></img>\n\n## Example 1 table of contents\n\n* [E1.1 Redis setup](#redis)<br>\n* [E1.2 Create a streaming pipeline](#create_p1) <br>\n* [E1.3 Process login clickstream events](#login) <br>\n* [E1.4 Process add_to_cart clickstream events](#addtocart) <br>\n* [E1.5 Process checkout clickstream events](#addtocart) <br>\n* [E1.6 Run the pipeline](#run_1)<br>\n* [E1.7 Summary and next steps](#summary_1)<br>\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"redis\"></a>\n***\n\n## E1.1 Redis setup\n\nRedis is an in-memory database. It stores its data in RAM, making it a very fast way of storing and retrieving data. It provides a set of primitive data structures, but we only concern ourselves with [hashes](https://redis.io/commands#hash) for this exercise.\n\nA Redis hash is a data structure that allows several keys to be stored together. We are going to configure a Redis hash called `funnel` that contains the following output:\n\n- login_count - the number of people who logged into LocalCart\n- basket_count - the number of items added into a shopping cart\n- checkout_count - the number of purchases made\n- basket_total - the total price of items added into a shopping cart\n- checkout_total - the total price of items purchased\n\nThese are the outputs of the aggregation functions in our streaming pipeline. \n\n\n### E1.1.1 Collect your Redis connection information\n\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">Bluemix Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned **Compose for Redis** service and click on the service instance name.\n1. Open the _Service Credentials_ tab and view the credentials.\n```\n{\n  \"db_type\": \"redis\",\n  \"maps\": [],\n  \"name\": \"b...b\",\n  \"uri_cli\": \"redis-cli -h **HOSTNAME** -p **PORT** -a **PASSWORD**\",\n  \"deployment_id\": \"5...2\",\n  \"uri\": \"redis://admin:**PASSWORD**@**HOSTNAME**:**PORT**\"\n}\n```\n\nNote your `**HOSTNAME**`, `**PORT**` and `**PASSWORD**` information.\n\n\n### E1.1.2 Verify your redis connectivity\nYou can verify your redis connectivity information in this notebook by installing the Python Redis library with the following command:"}, {"cell_type": "code", "outputs": [], "source": "!pip install redis", "metadata": {"collapsed": true}, "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "We import the library and connect to Redis with the following command. Replace the credential placeholders with your credentials."}, {"cell_type": "code", "outputs": [], "source": "import redis\n# TODO replace **HOSTNAME**, **PORT** and **PASSWORD** with your credentials\nr = redis.StrictRedis(host='**HOSTNAME**', port=**PORT**, db=0, password='**PASSWORD**')", "metadata": {"collapsed": true}, "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "We can then create a hash called `funnel` to store our real-time data to the database by using the `hset` function:"}, {"cell_type": "code", "outputs": [], "source": "r.hset('funnel', 'basket_count', 554);\nr.hset('funnel', 'basket_total', 951);\nr.hset('funnel', 'checkout_count', 21);\nr.hset('funnel', 'checkout_total', 5400);\nr.hset('funnel', 'login_count', 100);", "metadata": {"collapsed": true}, "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "We can also use this connection to retrieve all the values from our `funnel` hash using `hgetall`:"}, {"cell_type": "code", "outputs": [], "source": "r.hgetall('funnel')", "metadata": {"collapsed": true}, "execution_count": null}, {"cell_type": "markdown", "metadata": {}, "source": "**Note:** \nThe Redis connection above seems to freeze in this notebook after a minute or so. In this case, you will need to restart the notebook kernel to restore it.\n<BR>\nWe can now create streaming pipelines that store aggregated data in Redis."}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"create_p1\"></a>\n***\n\n## E1.2 Create a streaming pipeline\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab and scroll to the _Streaming Pipelines_ section.\n1. Click **+ add streaming pipelines**.\n1. In the _Create Streaming Pipeline_ window, \n  1. Enter pipeline name `aggregate_for_redis`\n  1. Select **Manual**. (You will use the wizard in Example 2.)\n  1. Click **Create**.\n\nAn empty canvas is displayed, along with a list of _Source_, _Target_ and _Processing and Analytics_ operators that you can choose from. Source operators load data and target operators store data.\n\n<a id=\"login\"></a>\n***\n\n## E1.3 Process login clickstream events\n\nFirst we need to collect `login` data from Message Hub and calculate the number of logins during a rolling one hour time window. The incoming `login` event payload has the following structure:\n```\n  {\n    \"customer_id\": \"13872\",\n    \"click_event_type\": \"login\",\n    \"total_price_of_basket\": \"0.0\",\n    \"total_number_of_items_in_basket\": \"0\",\n    \"total_number_of_distinct_items_in_basket\": \"0\",\n    \"event_time\": \"2017-07-11 20:10:52 UTC\"\n  }\n```\n\n\n### E1.3.1 Configure the source\n\n1. Drag a **MessageHub** source operator into the pipeline canvas.\n1. Configure the MessageHub operator by doing these steps in the _Properties_ pane:\n\t1. Select your Message Hub instance.\n\t1. Select the `login` topic.\n\t1. Click **Edit Schema** to specify the payload properties this operator will make available to operators that are connected to its output port. Since we only want to count the number of login events we only make the `customer_id` available.\n    1. Choose\n            - Attribute Name: `customer_id`\n            - Type: `Number` \n            - JSON Path: `/customer_id` \n    1. Click **Save** and **Close**.         \n\n\nOur streaming pipeline now has its first operator and looks like this: \n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/M.png'></img>\n\n\n### E1.3.2 Set up aggregation functions\n\nStreaming data can be aggregated by applying functions such as sum, count, minimum, or maximum. The results of the aggregation can be done on the aggregation before it is written to the Redis database. Our aim is to calculate the number of people who logged into LocalCart for a sliding one-hour window.\n\nIn the pipeline canvas, do these steps:\n\n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the MessageHub operator.\n2. Drag your mouse pointer from the output port of the MessageHub operator to the input port of the Aggregation operator to connect them.\n3. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n4. In the **Functions** area of the _Aggregation Properties_ pane, define one aggregation:\n    - Aggregation 1: count the logins\n        - Output Field Name - `login_count`\n        - Function Type - `Count`\n        \n    Note: To identify how many different customers have logged in during the rolling 1 hour time window, we would use the `CountDistinct` function and apply it to `customer_id`.\n\nOur pipeline now has two connected operators: a source operator and an aggregation operator. Hover over the arrow to review the data flow between them.\n\n<img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/op_op_io.png'></img>\n\n\n\n### E1.3.3 Configure the target\n\nNext, add a Redis target operator. In the streaming pipeline canvas, do these steps:\n\n1. Drag a **Redis** operator from the _Target_ area, and then drop it on the canvas next to the Aggregation operator.\n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n1. Click the **Redis** operator to open its Properties pane. \n    - Type in the `**HOSTNAME**`, `**PORT**` and `**PASSWORD**` credentials of your Redis by Compose service.\n    - In the **Key Template** field, type in `funnel`. \n    - Click **Test Connection** to validate that your connection information is correct.\n1. Save the pipeline. The setup for `login` event processing is complete.\n\n  <img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/MAR.png'></img>    \n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "***"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"addtocart\"></a>\n## E1.4 Process add_to_cart clickstream events\n\nNext we need to collect `add_to_cart` event data from Message Hub and calculate the number of shopping baskets and their combined value during a rolling one hour time window. The incoming `add_to_cart` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"13859\",\n    \"click_event_type\": \"add_to_cart\",\n    \"product_name\": \"Oatmeal\",\n    \"product_category\": \"Food\",\n    \"product_price\": \"2.49\",\n    \"total_price_of_basket\": \"153.41\",\n    \"total_number_of_items_in_basket\": \"19\",\n    \"total_number_of_distinct_items_in_basket\": \"6\",\n    \"event_time\": \"2017-06-23 12:56:18 UTC\"\n}\n```\n\n### E1.4.1 Configure the source, aggregation function and target for add_to_cart events\n\n1. Drag another **MessageHub** source operator into the pipeline canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select your MessageHub instance.\n\t1. Select the `add_to_cart` topic.\n\t1. Click **Edit Schema** to make the customer id and cart value available to connected operators. \n    1. The message schema can be automatically detected if a producer has already generated messages for the selected topic. Click **Detect Schema** and **Show preview**.\n      > If no messsages are displayed and the schema is not populated verify that your producer is running.\n    1. Click **Hide Preview**.\n    1. Remove all attributes except `customer_id` and `total_price_of_basket`:\n      - Attribute Name: `customer_id`\n            - Type: `Number` \n            - JSON Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - JSON Path: `/total_price_of_basket` \n    1. Click **Save** and **Close**.\n1. Drag an **Aggregation** operator from the **Processing and Analytics** area, and then drop it on the canvas next to the MessageHub operator.\n1. Drag your mouse pointer from the output port of the MessageHub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count the baskets\n        - Output Field Name - `basket_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum up basket values\n        - Output Field Name - `basket_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them.\n\n Your pipeline is now configured to stream and aggregate `login` and `add_to_cart` events:\n    \n <img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/MARMAR.png'></img>    \n\n1. Save your pipeline. No errors should be reported.\n\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"checkout\"></a>\n***\n\n## E1.5 Process checkout clickstream events\n\nFirst we need to create a streaming pipeline that collects `checkout` event data from a Message Hub operator and calculates the number of checkouts and their combined value during a rolling one hour time window. The incoming `checkout` event payload has the following structure:\n\n```\n{\n    \"customer_id\": \"11828\",\n    \"click_event_type\": \"checkout\",\n    \"total_price_of_basket\": \"72.80000000000001\",\n    \"total_number_of_items_in_basket\": \"20\",\n    \"total_number_of_distinct_items_in_basket\": \"5\",\n    \"session_duration\": \"440\",\n    \"event_time\": \"2017-06-23 13:09:12 UTC\"\n}\n```\n\n### E1.5.1 Set up pipeline source, aggregation function and target for checkout events\n\n1. Drag another **MessageHub** source operator into the pipeline canvas.\n1. Configure the MessageHub operator by doing these steps in the Properties pane:\n\t1. Select the ClickStream MessageHub instance.\n\t1. Select the `checkout` topic.\n\t1. Click **Edit Schema** to specify the message attributes that this pipeline will consume. Define the following attributes (by entering them manually or customizing the auto-detected schema):\n      - Attribute Name: `customer_id` \n            - Type: `Number` \n            - JSON Path: `/customer_id` \n      - Attribute Name: `total_price_of_basket` \n            - Type: `Number` \n            - JSON Path: `/total_price_of_basket` \n1. Drag an **Aggregation** operator from the _Processing and Analytics_ area, and then drop it on the canvas next to the MessageHub operator.\n1. Drag your mouse pointer from the output port of the MessageHub operator to the input port of the Aggregation operator to connect them.\n1. Click the **Aggregation** operator to open its _Properties_ pane. Set the following _Aggregation Window_ parameters:\n    - Type - `sliding`\n    - Time Units - `hour`\n    - Number of Time Units - `1`\n    - Partition By - leave unchanged\n    - Group By - leave unchanged\n1. In the **Functions** area of the _Aggregation Properties_ pane, define two aggregations:\n    - Aggregation 1: count checkouts\n        - Output Field Name - `checkout_count`\n        - Function Type - `Count`\n    - Aggregation 2: Sum basket values\n        - Output Field Name - `checkout_total`\n        - Function Type - `Sum`\n        - Apply Function to - `total_price_of_basket`\n        \n1. Copy the existing **Redis** operator that's already on the canvas and paste it next to the _Aggregation_ Operator. \n1. Drag your mouse pointer from the output port of the Aggregation operator to the input port of the Redis operator to connect them. The completed pipeline now looks as follows: <br>\n   <img src='https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/MARMARMAR.png'></img>    \n\n1. Save your pipeline. No errors should be reported.\n\n<a id=\"run_1\"></a>\n## E1.6 Run the pipeline\n\n1. To run the pipeline, click **Run**. \n1. Wait for the pipeline to start. If the pipeline does not start verify your pipeline setup. If no events are flowing from MessageHub Operators make sure that your producer (simulating user activity), which you've launched in notebook 1, is running. \n1. Click on any operator to display throughput information.\n\n<img src= \"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb2_redis_streaming_status.png\"></img>\n\nCongratulations! You just created a streaming pipeline that ingests clickstream data from Message Hub, aggregates data and stores it in Redis storage.\n\n\n<a id=\"summary_1\"></a>\n## E1.7 Summary and next steps\nIn this section, you consumed and aggregated clickstream events that were generated in [Notebook #1: Creating a Kafka Producer of ClickStream events](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-1.ipynb).\n\nYou can now skip to [Notebook#4:Visualize streaming data in a real-time dashboard](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-4.ipynb) to learn about how to visualize the aggregated data in real-time using a simple web application or continue to the next section to configure a pipeline for static analysis.\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb2_dashboard.png\"></img>\n\n"}, {"cell_type": "markdown", "metadata": {}, "source": "<a id=\"intro_b\"></a>\n\n***\n# Example 2: Capturing clickstream events for static analysis\n***\n\n\nIn this second example you will create multiple pipelines that ingest all clickstream events and store them as-is in CSV files on Object Storage in preparation for static analysis, as illustrated in [Notebook#3b: Analyze static clickstreams](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb).\n\n<img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/static_analysis.png\"></img>\n\n\n## Example 2 table of contents\n\n* [E2.1 Create a pipeline for login clickstream events](#login_2) <br>\n* [E2.2 Create additional pipelines](#more_2) <br>\n* [E2.3 Summary and next steps](#summary_2)<br>\n\n\n<a id=\"login_2\"></a>\n***\n\n## E2.1 Create a pipeline for login clickstream events\n\nIn IBM Data Science Experience, do these steps:\n\n1. Select a project that you want to contain the streaming pipeline.\n1. Click the **Analytics Assets** tab and scroll to the _Streaming Pipelines_ section.\n1. Click **+ add streaming pipelines**.\n1. In the _Create Streaming Pipeline_ window, \n  1. Enter pipeline name `store_login_on_os`.\n  1. Select **Wizard**. \n  1. Click **Create**.\n1. In the _Select Source_ tab click **Message Hub**.\n1. Under the Instance drop-down menu, select your Message Hub instance.\n1. Under the Topic drop-down menu, select **login** and click **Continue**.\n1. Wait for the Data Preview window to display the streaming data for the selected event. (If no data is displayed make sure your producer is running.)\n > You can customize the pre-defined schema (e.g. remove attributes or change data types) by clicking _Edit schema_. Do not make any changes at this time.\n1. Click **Continue**.\n1. In the Select Target window, click **Object Storage**.\n1. Under the Object Storage Instance drop-down menu, select the instance that is used by the DSX project.\n   <br>\n   > Take note of the  Object Storage instance name. You will need this information in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb) when you load and analyze the clickstream events.\n1. Under the Container drop-down menu, select the Object Storage container you want to write to. \n   <br>\n   > Take note of the  Object Storage container name. You will need this information in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb) when you load and analyze the clickstream events.\n1. Under _File Name_, type **login_TIMESTAMP.csv** (**Note:** \"TIMESTAMP\" is a reserved word that will be replaced with an actual timestamp when the file is written).\n   > Note: if you choose a file name other than `login_TIMESTAMP.csv` you must also modify notebook 3B and change the default file name in the data load cell.\n1. Under _Format_, select **csv**.\n1. Under _Delimiter_, select **Comma (,)**.\n1. Click **Continue** and **Save**. \n1. Click **Run** to start the pipeline. If no errors are reported all login events are written to timestamped CSV files in Object Storage.\n      <img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb2_login_monitoring.png\"></img>\n1. Click **Stop** to stop the pipeline.\n\nYou can backup a pipeline by downloading/exporting it.\n\n<a id=\"more_2\"></a>\n***\n\n## E2.2 Create additional pipelines\n\n\nIn preparation for this example scenario we have already created pipelines that capture the `add_to_cart`, `browsing`, `checkout`, `login`, `logout_with_purchase`, and `logout_without_purchase` clickstream events and save them to Object Storage. \n\n\n1. Download the following pipeline file to your local machine: https://raw.githubusercontent.com/wdp-beta/get-started/master/pipelines/store_events_on_os.stp\n1. Click the **Analytics Assets** tab and scroll to the _Streaming Pipelines_ section.\n1. Click **+ add streaming pipelines**.\n1. In the _Create Streaming Pipeline_ window, \n   * Select **From File**.\n   * Drag and drop a pipeline file or browse to it. The pipeline name and description are populated.\n   * Click **Create**. The pipeline is displayed.\n    > Note that the _Notifications_ icons is highlighted and the run button disabled, indicating that there are issues.\n   * Click the **Notifications** icon and click on the **Pipeline is misconfigured** error. The pipeline canvas is displayed.   \n1. Click the **Notifications** icon again to open the error details. It appears that the configurations for the MessageHub operators and Object Storage operators are invalid. This is expected because the service instances that were used when the pipeline was created are not available to you.\n1. Repeat the following steps for each MessageHub operator on the canvas:\n    * Click on the MessageHub operator to open its properties.\n    * Select your MessageHub instance. The appropriate topic is automatically selected if your MessageHub instance subscribes to it.\n    * No additional changes are required for this operator.\n1. Repeat the following steps for each Object Storage operator on the canvas:\n    * Click on the Object Storage operator to open its properties.    \n    * Select your Object Storage instance. \n    * Select the Object Storage container where you want to store the CSV file.\n    * No additional changes are required for this operator.\n1. Save the pipeline. No errors should be reported.\n1. Click **Run** to start the pipelines. Six pipelines should be running, capturing `add_to_cart`, `browsing`, `checkout`, `login`, `logout_with_purchase`, and `logout_without_purchase` events and storing them in CSV files on Object Storage:\n <img src=\"https://raw.githubusercontent.com/wdp-beta/get-started/master/notebooks/images/nb2_csv_pipelines_monitoring.png\"></img>\n\n\n<a id=\"summary_2\"></a>\n***\n\n## E2.3 Summary and next steps\n\nYou can now do one of the following:\n\n#### Accessing CSV files on Object Storage\n1. Open your <a target=\"_blank\" href=\"https://apsportal.ibm.com/settings/services?context=analytics\">Bluemix Data Services list</a>. A list of your provisioned services is displayed.\n1. Locate the pre-provisioned Object Storage service and click on the service instance name.\n\n#### Accessing CSV files on Object Storage manually\n1. Open the **Manage** tab, and then select the container that you specified when you created the data collection pipeline. \n1. Select a CSV file. In the \"Select Action\"\" list, select \"Download File\" to view it.\n\n#### Accessing CSV files on Object Storage programatically\n1. Open the **Service credentials** tab. Select a Key Name, and then click **View credentials**. \n1. Copy the credentials and provide this information whenever you want to load data files programatically, such as in [Notebook 3b: Static clickstream analysis](https://github.com/wdp-beta/get-started/blob/master/notebooks/localcart-scenario-part-3b.ipynb).\n"}, {"cell_type": "markdown", "metadata": {}, "source": "\n***\n\n### Authors\n\nGlynn Bird is a Developer Advocate for Watson Data Platform at IBM. \n\nRaj Singh is a Developer Advocate for Watson Data Platform at IBM.\n\n***\nCopyright \u00a9 IBM Corp. 2017. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"display_name": "Python 2 with Spark 2.0", "name": "python2-spark20", "language": "python"}, "language_info": {"codemirror_mode": {"version": 2, "name": "ipython"}, "version": "2.7.11", "nbconvert_exporter": "python", "mimetype": "text/x-python", "file_extension": ".py", "name": "python", "pygments_lexer": "ipython2"}}, "nbformat_minor": 1}